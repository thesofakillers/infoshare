#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --job-name=DEP
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=01:00:00
#SBATCH --mem-per-gpu=32000M
#SBATCH --output=lisa/outputs/dep_train_%A.out

module purge
module load 2022
module load Anaconda3/2022.05

source activate bert-infoshare

srun python -u infoshare/run/train.py --task DEP --num_workers 18 \
    --encoder_name xlm-roberta-base \
    --treebank_name en_gum \
    --concat_mode ONLY \
    --aggregation first \
    --probe_layer 9
