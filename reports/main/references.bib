@article{choenni_investigating_2022,
	title = {Investigating {Language} {Relationships} in {Multilingual} {Sentence} {Encoders} {Through} the {Lens} of {Linguistic} {Typology}},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00444},
	doi = {10.1162/coli_a_00444},
	abstract = {Multilingual sentence encoders have seen much success in cross-lingual model transfer for downstream NLP tasks. The success of this transfer is, however, dependent on the model’s ability to encode the patterns of cross-lingual similarity and variation. Yet, we know relatively little about the properties of individual languages or the general patterns of linguistic variation that the models encode. In this article, we investigate these questions by leveraging knowledge from the field of linguistic typology, which studies and documents structural and semantic variation across languages. We propose methods for separating language-specific subspaces within state-of-the-art multilingual sentence encoders (LASER, M-BERT, XLM, and XLM-R) with respect to a range of typological properties pertaining to lexical, morphological, and syntactic structure. Moreover, we investigate how typological information about languages is distributed across all layers of the models. Our results show interesting differences in encoding linguistic variation associated with different pretraining strategies. In addition, we propose a simple method to study how shared typological properties of languages are encoded in two state-of-the-art multilingual models—M-BERT and XLM-R. The results provide insight into their information sharing mechanisms and suggest that these linguistic properties are encoded jointly across typologically similar languages in these models.},
	urldate = {2022-05-25},
	journal = {Computational Linguistics},
	author = {Choenni, Rochelle and Shutova, Ekaterina},
	month = may,
	year = {2022},
	pages = {1--38},
	file = {Choenni_Shutova_2022_Investigating Language Relationships in Multilingual Sentence Encoders Through.pdf:/home/dinos/Zotero/storage/P38LN8V8/Choenni_Shutova_2022_Investigating Language Relationships in Multilingual Sentence Encoders Through.pdf:application/pdf;Full Text PDF:/home/dinos/Zotero/storage/52Y7EINX/Choenni and Shutova - 2022 - Investigating Language Relationships in Multilingu.pdf:application/pdf;Snapshot:/home/dinos/Zotero/storage/VTL8VMZ7/Investigating-language-relationships-in.html:text/html},
}


@inproceedings{tenney_what_2018,
  title = {What Do You Learn from Context? {{Probing}} for Sentence Structure in Contextualized Word Representations},
  shorttitle = {What Do You Learn from Context?},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
  year = {2018},
  month = sep,
  langid = {english},
  keywords = {inspiration for probing classifier},
  file = {/Users/thesofakillers/Zotero/storage/BG6HDZAL/Tenney et al_2018_What do you learn from context.pdf;/Users/thesofakillers/Zotero/storage/K495SMRS/forum.html}
}








@inproceedings{lee_end--end_2017,
	address = {Copenhagen, Denmark},
	title = {End-to-end {Neural} {Coreference} {Resolution}},
	url = {https://aclanthology.org/D17-1018},
	doi = {10.18653/v1/D17-1018},
	abstract = {We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.},
	urldate = {2022-05-02},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
	month = sep,
	year = {2017},
	keywords = {self attention pooling for span representations},
	pages = {188--197},
	file = {Lee et al_2017_End-to-end Neural Coreference Resolution.pdf:/Users/matteorosati/Zotero/storage/3C979EBH/Lee et al_2017_End-to-end Neural Coreference Resolution.pdf:application/pdf},
}

@article{choenni_what_2020,
	title = {What does it mean to be language-agnostic? {Probing} multilingual sentence encoders for typological properties},
	shorttitle = {What does it mean to be language-agnostic?},
	url = {http://arxiv.org/abs/2009.12862},
	abstract = {Multilingual sentence encoders have seen much success in cross-lingual model transfer for downstream NLP tasks. Yet, we know relatively little about the properties of individual languages or the general patterns of linguistic variation that they encode. We propose methods for probing sentence representations from state-of-the-art multilingual encoders (LASER, M-BERT, XLM and XLM-R) with respect to a range of typological properties pertaining to lexical, morphological and syntactic structure. In addition, we investigate how this information is distributed across all layers of the models. Our results show interesting differences in encoding linguistic variation associated with different pretraining strategies.},
	urldate = {2022-05-02},
	journal = {arXiv:2009.12862 [cs]},
	author = {Choenni, Rochelle and Shutova, Ekaterina},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.12862},
	keywords = {Computer Science - Computation and Language, katia and rochelle's slightly older paper},
	file = {arXiv.org Snapshot:/Users/matteorosati/Zotero/storage/GXJHLCG5/2009.html:text/html;Choenni_Shutova_2020_What does it mean to be language-agnostic.pdf:/Users/matteorosati/Zotero/storage/WVVNRHGD/Choenni_Shutova_2020_What does it mean to be language-agnostic.pdf:application/pdf},
}


@inproceedings{tenney_bert_2019,
	address = {Florence, Italy},
	title = {{BERT} {Rediscovers} the {Classical} {NLP} {Pipeline}},
	url = {https://aclanthology.org/P19-1452},
	doi = {10.18653/v1/P19-1452},
	abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
	urldate = {2022-05-18},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
	month = jul,
	year = {2019},
	pages = {4593--4601},
	file = {Full Text PDF:/home/dinos/Zotero/storage/JRNP7VQ4/Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:application/pdf},
}


@article{wolf_transfertransfo_2019,
	title = {{TransferTransfo}: {A} {Transfer} {Learning} {Approach} for {Neural} {Network} {Based} {Conversational} {Agents}},
	shorttitle = {{TransferTransfo}},
	url = {http://arxiv.org/abs/1901.08149},
	abstract = {We introduce a new approach to generative data-driven dialogue systems (e.g. chatbots) called TransferTransfo which is a combination of a Transfer learning based training scheme and a high-capacity Transformer model. Fine-tuning is performed by using a multi-task objective which combines several unsupervised prediction tasks. The resulting fine-tuned model shows strong improvements over the current state-of-the-art end-to-end conversational models like memory augmented seq2seq and information-retrieval models. On the privately held PERSONA-CHAT dataset of the Conversational Intelligence Challenge 2, this approach obtains a new state-of-the-art, with respective perplexity, Hits@1 and F1 metrics of 16.28 (45 \% absolute improvement), 80.7 (46 \% absolute improvement) and 19.5 (20 \% absolute improvement).},
	urldate = {2022-05-10},
	journal = {arXiv:1901.08149 [cs]},
	author = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
	month = feb,
	year = {2019},
	note = {arXiv: 1901.08149},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/matteorosati/Zotero/storage/KIJ3LU8Y/Wolf et al. - 2019 - TransferTransfo A Transfer Learning Approach for .pdf:application/pdf;arXiv.org Snapshot:/Users/matteorosati/Zotero/storage/8ZS649YQ/1901.html:text/html},
}


@inproceedings{conneau_what_2018,
	address = {Melbourne, Australia},
	title = {What you can cram into a single \$\&!\#* vector: {Probing} sentence embeddings for linguistic properties},
	shorttitle = {What you can cram into a single \$\&!\#* vector},
	url = {https://aclanthology.org/P18-1198},
	doi = {10.18653/v1/P18-1198},
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	urldate = {2022-05-18},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
	month = jul,
	year = {2018},
	pages = {2126--2136},
	file = {Full Text PDF:/home/dinos/Zotero/storage/E28J6VF9/Conneau et al. - 2018 - What you can cram into a single \$&!# vector Prob.pdf:application/pdf},
}


@article{pires_how_2019,
	title = {How multilingual is {Multilingual} {BERT}?},
	url = {http://arxiv.org/abs/1906.01502},
	abstract = {In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.},
	urldate = {2022-05-10},
	journal = {arXiv:1906.01502 [cs]},
	author = {Pires, Telmo and Schlinger, Eva and Garrette, Dan},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01502},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/matteorosati/Zotero/storage/XXFWBGQJ/Pires et al. - 2019 - How multilingual is Multilingual BERT.pdf:application/pdf;arXiv.org Snapshot:/Users/matteorosati/Zotero/storage/WY7AVZ6Z/1906.html:text/html},
}


@inproceedings{clark_what_2019,
	address = {Florence, Italy},
	title = {What {Does} {BERT} {Look} at? {An} {Analysis} of {BERT}’s {Attention}},
	shorttitle = {What {Does} {BERT} {Look} at?},
	url = {https://www.aclweb.org/anthology/W19-4828},
	doi = {10.18653/v1/W19-4828},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classiﬁers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, speciﬁc positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we ﬁnd heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classiﬁer and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.},
	language = {en},
	urldate = {2022-05-18},
	booktitle = {Proceedings of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	year = {2019},
	pages = {276--286},
	file = {Clark et al. - 2019 - What Does BERT Look at An Analysis of BERT’s Atte.pdf:/home/dinos/Zotero/storage/ADUN7Z9V/Clark et al. - 2019 - What Does BERT Look at An Analysis of BERT’s Atte.pdf:application/pdf},
}


@inproceedings{ravishankar_multilingual_2019,
	address = {Turku, Finland},
	title = {Multilingual {Probing} of {Deep} {Pre}-{Trained} {Contextual} {Encoders}},
	url = {https://aclanthology.org/W19-6205},
	abstract = {Encoders that generate representations based on context have, in recent years, benefited from adaptations that allow for pre-training on large text corpora. Earlier work on evaluating fixed-length sentence representations has included the use of `probing' tasks, that use diagnostic classifiers to attempt to quantify the extent to which these encoders capture specific linguistic phenomena. The principle of probing has also resulted in extended evaluations that include relatively newer word-level pre-trained encoders. We build on probing tasks established in the literature and comprehensively evaluate and analyse – from a typological perspective amongst others – multilingual variants of existing encoders on probing datasets constructed for 6 non-English languages. Specifically, we probe each layer of a multiple monolingual RNN-based ELMo models, the transformer-based BERT's cased and uncased multilingual variants, and a variant of BERT that uses a cross-lingual modelling scheme (XLM).},
	urldate = {2022-05-10},
	booktitle = {Proceedings of the {First} {NLPL} {Workshop} on {Deep} {Learning} for {Natural} {Language} {Processing}},
	publisher = {Linköping University Electronic Press},
	author = {Ravishankar, Vinit and Gökırmak, Memduh and Øvrelid, Lilja and Velldal, Erik},
	month = sep,
	year = {2019},
	pages = {37--47},
	file = {Full Text PDF:/Users/matteorosati/Zotero/storage/6TEJWY5H/Ravishankar et al. - 2019 - Multilingual Probing of Deep Pre-Trained Contextua.pdf:application/pdf},
}



@article{chi_finding_2020,
	title = {Finding {Universal} {Grammatical} {Relations} in {Multilingual} {BERT}},
	doi = {10.18653/v1/2020.acl-main.493},
	abstract = {An unsupervised analysis method is presented that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy, suggesting that even without explicit supervision, multilingual masked language models learn certain linguistic universals. Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks’ internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.},
	journal = {ACL},
	author = {Chi, Ethan A. and Hewitt, John and Manning, Christopher D.},
	year = {2020},
	file = {Full Text:/Users/matteorosati/Zotero/storage/DSN6RHMZ/Chi et al. - 2020 - Finding Universal Grammatical Relations in Multili.pdf:application/pdf},
}

@inproceedings{libovicky_language_2020,
	address = {Online},
	title = {On the {Language} {Neutrality} of {Pre}-trained {Multilingual} {Representations}},
	url = {https://aclanthology.org/2020.findings-emnlp.150},
	doi = {10.18653/v1/2020.findings-emnlp.150},
	abstract = {Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead investigate the language-neutrality of multilingual contextual embeddings directly and with respect to lexical semantics. Our results show that contextual embeddings are more language-neutral and, in general, more informative than aligned static word-type embeddings, which are explicitly trained for language neutrality. Contextual embeddings are still only moderately language-neutral by default, so we propose two simple methods for achieving stronger language neutrality: first, by unsupervised centering of the representation for each language and second, by fitting an explicit projection on small parallel data. Besides, we show how to reach state-of-the-art accuracy on language identification and match the performance of statistical methods for word alignment of parallel sentences without using parallel data.},
	urldate = {2022-05-11},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Libovický, Jindřich and Rosa, Rudolf and Fraser, Alexander},
	month = nov,
	year = {2020},
	pages = {1663--1674},
	file = {Full Text PDF:/Users/matteorosati/Zotero/storage/LYYGPUXH/Libovický et al. - 2020 - On the Language Neutrality of Pre-trained Multilin.pdf:application/pdf},
}

@inproceedings{hewitt_structural_2019,
	address = {Minneapolis, Minnesota},
	title = {A {Structural} {Probe} for {Finding} {Syntax} in {Word} {Representations}},
	url = {https://aclanthology.org/N19-1419},
	doi = {10.18653/v1/N19-1419},
	abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.},
	urldate = {2022-05-11},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Manning, Christopher D.},
	month = jun,
	year = {2019},
	pages = {4129--4138},
	file = {Full Text PDF:/Users/matteorosati/Zotero/storage/67QNSZXT/Hewitt and Manning - 2019 - A Structural Probe for Finding Syntax in Word Repr.pdf:application/pdf},
}


@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-05-11},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/Users/matteorosati/Zotero/storage/273LJDZI/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}


@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2022-05-11},
	journal = {arXiv:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.11692},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/matteorosati/Zotero/storage/IHGBLUMD/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf},
}

@inproceedings{wu_are_2020,
	address = {Online},
	title = {Are {All} {Languages} {Created} {Equal} in {Multilingual} {BERT}?},
	url = {https://aclanthology.org/2020.repl4nlp-1.16},
	doi = {10.18653/v1/2020.repl4nlp-1.16},
	abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
	urldate = {2022-05-11},
	booktitle = {Proceedings of the 5th {Workshop} on {Representation} {Learning} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Shijie and Dredze, Mark},
	month = jul,
	year = {2020},
	pages = {120--130},
	file = {Full Text PDF:/Users/matteorosati/Zotero/storage/LBJQVATH/Wu and Dredze - 2020 - Are All Languages Created Equal in Multilingual BE.pdf:application/pdf},
}

@article{sahin_linspector_2020,
	title = {{LINSPECTOR}: {Multilingual} {Probing} {Tasks} for {Word} {Representations}},
	volume = {46},
	shorttitle = {{LINSPECTOR}},
	url = {https://aclanthology.org/2020.cl-2.4},
	doi = {10.1162/coli_a_00376},
	abstract = {Despite an ever-growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation that requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: The information encoded by the word order and function words in English is often stored on a subword, morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count, and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting, which is challenging because of a lack of resources, lower quality of tools, and differences among languages. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition, and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting. We release the probing data sets and the evaluation suite LINSPECTOR with https://github.com/UKPLab/linspector.},
	number = {2},
	urldate = {2022-05-11},
	journal = {Computational Linguistics},
	author = {Şahin, Gözde Gül and Vania, Clara and Kuznetsov, Ilia and Gurevych, Iryna},
	month = jun,
	year = {2020},
	pages = {335--385},
	file = {Full Text PDF:/Users/matteorosati/Zotero/storage/BPB85APF/Şahin et al. - 2020 - LINSPECTOR Multilingual Probing Tasks for Word Re.pdf:application/pdf},
}

@inproceedings{blevins_deep_2018,
	address = {Melbourne, Australia},
	title = {Deep {RNNs} {Encode} {Soft} {Hierarchical} {Syntax}},
	url = {https://aclanthology.org/P18-2003},
	doi = {10.18653/v1/P18-2003},
	abstract = {We present a set of experiments to demonstrate that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. We consider four syntax tasks at different depths of the parse tree; for each word, we predict its part of speech as well as the first (parent), second (grandparent) and third level (great-grandparent) constituent labels that appear above it. These predictions are made from representations produced at different depths in networks that are pretrained with one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision.},
	urldate = {2022-05-11},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
	month = jul,
	year = {2018},
	pages = {14--19},
	file = {Full Text PDF:/Users/matteorosati/Zotero/storage/9B9LS5YQ/Blevins et al. - 2018 - Deep RNNs Encode Soft Hierarchical Syntax.pdf:application/pdf},
}



@inproceedings{conneau_unsupervised_2020,
	address = {Online},
	title = {Unsupervised {Cross}-lingual {Representation} {Learning} at {Scale}},
	url = {https://aclanthology.org/2020.acl-main.747},
	doi = {10.18653/v1/2020.acl-main.747},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.},
	urldate = {2022-05-18},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2020},
	pages = {8440--8451},
	file = {Full Text PDF:/home/dinos/Zotero/storage/HFG7W2QU/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf:application/pdf},
}


@inproceedings{kudo_sentencepiece_2018,
	address = {Brussels, Belgium},
	title = {{SentencePiece}: {A} simple and language independent subword tokenizer and detokenizer for {Neural} {Text} {Processing}},
	shorttitle = {{SentencePiece}},
	url = {https://aclanthology.org/D18-2012},
	doi = {10.18653/v1/D18-2012},
	abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
	urldate = {2022-05-18},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Kudo, Taku and Richardson, John},
	month = nov,
	year = {2018},
	pages = {66--71},
	file = {Full Text PDF:/Users/toliz/Zotero/storage/2TFEWAQJ/Kudo and Richardson - 2018 - SentencePiece A simple and language independent s.pdf:application/pdf},
}



@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {https://aclanthology.org/P16-1162},
	doi = {10.18653/v1/P16-1162},
	urldate = {2022-05-18},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = aug,
	year = {2016},
	pages = {1715--1725},
	file = {Full Text PDF:/home/dinos/Zotero/storage/ECGDFXZP/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}


@inproceedings{nivre_universal_2020,
	address = {Marseille, France},
	title = {Universal {Dependencies} v2: {An} {Evergrowing} {Multilingual} {Treebank} {Collection}},
	isbn = {979-10-95546-34-4},
	shorttitle = {Universal {Dependencies} v2},
	url = {https://aclanthology.org/2020.lrec-1.497},
	abstract = {Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. The annotation consists in a linguistically motivated word segmentation; a morphological layer comprising lemmas, universal part-of-speech tags, and standardized morphological features; and a syntactic layer focusing on syntactic relations between predicates, arguments and modifiers. In this paper, we describe version 2 of the universal guidelines (UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of the currently available treebanks for 90 languages.},
	language = {English},
	urldate = {2022-05-18},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Hajič, Jan and Manning, Christopher D. and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
	month = may,
	year = {2020},
	pages = {4034--4043},
	file = {Full Text PDF:/home/dinos/Zotero/storage/KC9PGH7Z/Nivre et al. - 2020 - Universal Dependencies v2 An Evergrowing Multilin.pdf:application/pdf},
}

@article{zeldes_gum_2017,
	title = {The {GUM} corpus: creating multilayer resources in the classroom},
	volume = {51},
	issn = {1574-0218},
	shorttitle = {The {GUM} corpus},
	url = {https://doi.org/10.1007/s10579-016-9343-x},
	doi = {10.1007/s10579-016-9343-x},
	abstract = {This paper presents the methodology, design principles and detailed evaluation of a new freely available multilayer corpus, collected and edited via classroom annotation using collaborative software. After briefly discussing corpus design for open, extensible corpora, five classroom annotation projects are presented, covering structural markup in TEI XML, multiple part of speech tagging, constituent and dependency parsing, information structural and coreference annotation, and Rhetorical Structure Theory analysis. Layers are inspected for annotation quality and together they coalesce to form a richly annotated corpus that can be used to study the interactions between different levels of linguistic description. The evaluation gives an indication of the expected quality of a corpus created by students with relatively little training. A multifactorial example study on lexical NP coreference likelihood is also presented, which illustrates some applications of the corpus. The results of this project show that high quality, richly annotated resources can be created effectively as part of a linguistics curriculum, opening new possibilities not just for research, but also for corpora in linguistics pedagogy.},
	language = {en},
	number = {3},
	urldate = {2022-05-18},
	journal = {Language Resources and Evaluation},
	author = {Zeldes, Amir},
	month = sep,
	year = {2017},
	keywords = {Classroom annotation, Coreference, Information structure, Multilayer corpora, Parsing, Treebank},
	pages = {581--612},
	file = {Full Text PDF:/home/dinos/Zotero/storage/2W62VZ9C/Zeldes - 2017 - The GUM corpus creating multilayer resources in t.pdf:application/pdf},
}


@misc{delmonte_vit_2017,
	title = {{VIT} – {Venice} {Italian} {Treebank}: {Syntactic} and {Quantitative} {Features}},
	shorttitle = {{VIT} – {Venice} {Italian} {Treebank}},
	abstract = {In this paper we will describe VIT (Venice Italian Treebank), created at the University of Venice. We will focus on the syntactic-semantic features and on the quantitative analysis of the data of our treebank comparing them to other treebanks. In general, we will try to substantiate the claim that treebanking grammars or parsers is dramatically dependent on the chosen treebank; and eventually this process seems to be dependent either from substantial factors such as the adopted linguistic framework for structural description or, ultimately, the described language.},
	author = {Delmonte, Rodolfo and Bristot, Antonella and Tonelli, Sara},
	year = {2017},
	file = {Citeseer - Full Text PDF:/home/dinos/Zotero/storage/CWTLWUDN/Delmonte et al. - VIT – Venice Italian Treebank Syntactic and Quant.pdf:application/pdf;Citeseer - Snapshot:/home/dinos/Zotero/storage/NP62SI7T/summary.html:text/html},
}


@inproceedings{prokopidis_universal_2017,
	address = {Gothenburg, Sweden},
	title = {Universal {Dependencies} for {Greek}},
	url = {https://aclanthology.org/W17-0413},
	urldate = {2022-05-18},
	booktitle = {Proceedings of the {NoDaLiDa} 2017 {Workshop} on {Universal} {Dependencies} ({UDW} 2017)},
	publisher = {Association for Computational Linguistics},
	author = {Prokopidis, Prokopis and Papageorgiou, Haris},
	month = may,
	year = {2017},
	pages = {102--106},
	file = {Full Text PDF:/home/dinos/Zotero/storage/UIFRQNIY/Prokopidis and Papageorgiou - 2017 - Universal Dependencies for Greek.pdf:application/pdf},
}


@inproceedings{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {https://openreview.net/forum?id=Bkg6RiCqY7},
	urldate = {2022-05-18},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}, {ICLR} 2019, {New} {Orleans}, {LA}, {USA}, {May} 6-9, 2019},
	publisher = {OpenReview.net},
	author = {Loshchilov, Ilya and Hutter, Frank},
	year = {2019},
}


@techreport{straka_evaluating_2019,
	title = {Evaluating {Contextualized} {Embeddings} on 54 {Languages} in {POS} {Tagging}, {Lemmatization} and {Dependency} {Parsing}},
	url = {http://arxiv.org/abs/1908.07448},
	abstract = {We present an extensive evaluation of three recently proposed methods for contextualized embeddings on 89 corpora in 54 languages of the Universal Dependencies 2.3 in three tasks: POS tagging, lemmatization, and dependency parsing. Employing the BERT, Flair and ELMo as pretrained embedding inputs in a strong baseline of UDPipe 2.0, one of the best-performing systems of the CoNLL 2018 Shared Task and an overall winner of the EPE 2018, we present a one-to-one comparison of the three contextualized word embedding methods, as well as a comparison with word2vec-like pretrained embeddings and with end-to-end character-level word embeddings. We report state-of-the-art results in all three tasks as compared to results on UD 2.2 in the CoNLL 2018 Shared Task.},
	number = {arXiv:1908.07448},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Straka, Milan and Straková, Jana and Hajič, Jan},
	month = aug,
	year = {2019},
	note = {arXiv:1908.07448 [cs]
version: 1
type: article},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/toliz/Zotero/storage/Q3MP7MYR/Straka et al. - 2019 - Evaluating Contextualized Embeddings on 54 Languag.pdf:application/pdf},
}
